# üß† Customer 360 Data Platform

An end-to-end data engineering pipeline to build a 360¬∞ view of customers using Apache Airflow, Jupyter notebooks, PySpark, and Snowflake ‚Äî all orchestrated in a Docker environment.

---

## üìå Key Features

- Automated ETL using **Apache Airflow + Papermill**
- Notebook-based modular tasks: **Extract**, **Transform**, **Load**
- Built with **Docker**, **PySpark**, and **Snowflake**
- CI/CD with **GitHub Actions**
- Easily extendable for **data quality checks**, **dashboards**, or **alerts**

---

## üõ†Ô∏è Tech Stack

| Layer        | Tool/Tech                     |
|-------------|-------------------------------|
| Orchestration | Apache Airflow               |
| Notebooks     | Jupyter + Papermill          |
| Data Engine   | PySpark                      |
| Cloud DB      | Snowflake                    |
| Scheduler     | Airflow + cron (optional)    |
| Containerization | Docker, Docker Compose    |
| CI/CD         | GitHub Actions               |

---

## üß± Folder Structure

```
customer360/
‚îÇ
‚îú‚îÄ‚îÄ airflow/                # Airflow environment
‚îÇ   ‚îú‚îÄ‚îÄ dags/               # DAG scripts
‚îÇ   ‚îú‚îÄ‚îÄ logs/               # Airflow logs
‚îÇ   ‚îî‚îÄ‚îÄ airflow.cfg         # Config (auto-generated)
‚îÇ
‚îú‚îÄ‚îÄ work/
‚îÇ   ‚îî‚îÄ‚îÄ etl/                # All Jupyter notebooks
‚îÇ       ‚îú‚îÄ‚îÄ extract.ipynb
‚îÇ       ‚îú‚îÄ‚îÄ transform.ipynb
‚îÇ       ‚îî‚îÄ‚îÄ load_to_snowflake.ipynb
‚îÇ
‚îú‚îÄ‚îÄ data/                   # Source data (CSV, logs)
‚îÇ   ‚îú‚îÄ‚îÄ customers.csv
‚îÇ   ‚îú‚îÄ‚îÄ tickets.csv
‚îÇ   ‚îî‚îÄ‚îÄ logs/
‚îÇ
‚îú‚îÄ‚îÄ staging/                # Intermediate parquet files
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```

---

## üîÅ ETL Workflow

```mermaid
flowchart TD
    A[Start DAG: customer360_etl] --> B[Extract Task]
    B --> B1[extract_crm() & extract_tickets() & extract_logs()]
    B1 --> C[Transform Task]
    C --> C1[PySpark clean & join]
    C1 --> D[Load Task]
    D --> D1[load_to_snowflake()]
    D1 --> E[End DAG]
```

---

## üöÄ Quickstart

### 1. Clone and Run

```bash
git clone https://github.com/your-username/customer360-etl
cd customer360-etl
docker-compose up -d
```

### 2. Open JupyterLab

Visit: `http://localhost:8888` (token auto-generated in logs)

### 3. Start Airflow

```bash
docker exec -it <airflow_container_name> airflow standalone
```

Or inside container:

```bash
airflow scheduler
airflow webserver
```

---

## üß™ Trigger DAG

Manually from CLI:

```bash
airflow dags trigger customer360_etl
```

Or schedule via UI (disable `schedule=None` in DAG if needed).

---

## üìù Notebooks

- **extract.ipynb**: loads CRM, support ticket, and log data
- **transform.ipynb**: joins, deduplicates, and enriches via PySpark
- **load_to_snowflake.ipynb**: uploads the results to Snowflake

Make sure these are runnable independently with the required paths!

---

## ‚ö†Ô∏è Troubleshooting Tips

- Create `staging/` directory before DAG run:
  ```bash
  mkdir -p /home/jovyan/work/staging
  ```

- Airflow not finding your DAG?
  ```bash
  airflow dags list
  airflow dags list-import-errors
  ```

- Use absolute paths in your notebook for CSVs and staging files.

---

## ‚öôÔ∏è GitHub Actions (CI/CD)

> CI/CD ensures that your DAGs, notebooks, and Docker setup are valid and deployable.

**Workflows (in `.github/workflows/`)**
- Lint Python code
- Validate Airflow DAGs
- Optional: push image to DockerHub or deploy to cloud

---

## ‚úÖ What's Done

- [x] Local Dockerized Airflow
- [x] Jupyter notebook ETL jobs
- [x] DAG working with Papermill
- [x] Debugged and tested with real data
- [x] Ready for GitHub Actions setup

---

## üß© Optional Enhancements

- Add **Great Expectations** or **dbt tests**
- Use **S3** or **GCS** instead of local staging
- Connect to **Snowflake securely via Secrets Manager**
- Add **alerts** or Slack notifications on failure

---

## üß† Author

Built by [Your Name](https://github.com/your-username)  
Feel free to fork or contribute ü§ù

---